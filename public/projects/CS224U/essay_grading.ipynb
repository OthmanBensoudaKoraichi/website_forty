{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xJEAIPfS6k9"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SPc6ihQ1-JUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"drive/MyDrive/model//\"\n",
        "XLNET_MODEL_PATH = \"drive/MyDrive/model/xlnet//\"\n",
        "ALBERT_MODEL_PATH = \"drive/MyDrive/model/albert//\""
      ],
      "metadata": {
        "id": "8nDP6pMfAQWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"drive/MyDrive//training_set_rel3.tsv\""
      ],
      "metadata": {
        "id": "p_hwTArIFL4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbgFZNQbXJs6"
      },
      "outputs": [],
      "source": [
        " pip install torch transformers scikit-learn pandas openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4Dc4Qd-yWPD"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    XLNetTokenizer, XLNetForSequenceClassification,\n",
        "    ElectraTokenizer, ElectraForSequenceClassification,\n",
        "    AlbertTokenizer, AlbertForSequenceClassification,\n",
        "    PreTrainedTokenizer, PreTrainedModel\n",
        ")\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import openai\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSTPaN1eS_7c"
      },
      "source": [
        "# Explore data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lEapmfZVvLI"
      },
      "source": [
        "Some very good visualizations that help us understand the dataset come from this notebook : https://github.com/Turanga1/Automated-Essay-Scoring/blob/master/0_EDA_and_Topic_Modeling_with_LDA.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eX0I1CEz_Ra"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH, sep='\\t', encoding='ISO-8859-1')\n",
        "df.rename(columns={'essay': 'essay','domain1_score' : 'grade'}, inplace=True)\n",
        "df = df[[\"essay_id\",\"essay_set\",\"essay\",\"grade\"]]\n",
        "\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aRhXskZT4oi"
      },
      "source": [
        "## Analyze the train_df_df_dfing set size per essay set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frReP1Bd0144"
      },
      "outputs": [],
      "source": [
        "df.groupby('essay_set').agg('count').plot.bar(y='essay', legend=False)\n",
        "plt.title('Number of essays per set')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyzi0guBT9Bo"
      },
      "source": [
        "## Analyze the distribution of words per essay set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJKTUuaO017m"
      },
      "outputs": [],
      "source": [
        "df['word_count'] = df['essay'].str.strip().str.split().str.len()\n",
        "df.hist(column='word_count', by='essay_set', bins=25, sharey=True, sharex=True, layout=(2, 4), figsize=(7,4), rot=0)\n",
        "plt.suptitle('Word count by essay set')\n",
        "plt.xlabel('Number of words')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhOvQYK5UfGr"
      },
      "outputs": [],
      "source": [
        "topic_number = 0\n",
        "fig, ax = plt.subplots(4,2, figsize=(8,10))\n",
        "for i in range(4):\n",
        "    for j in range(2):\n",
        "        topic_number += 1\n",
        "        sns.violinplot(x='grade', y='word_count', data=df[df['essay_set'] == topic_number], ax=ax[i,j])\n",
        "        ax[i,j].set_title('Topic %i' % topic_number)\n",
        "ax[3,0].locator_params(nbins=10)\n",
        "ax[3,1].locator_params(nbins=10)\n",
        "plt.suptitle('Word count by score')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDA6bpiWWUNb"
      },
      "source": [
        "It appears that longer essays tend to score better for all essay sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2P8rYfHVcJK"
      },
      "outputs": [],
      "source": [
        "essay_set_number = 0\n",
        "fig, ax = plt.subplots(4,2, figsize=(9,9), sharey=False)\n",
        "for i in range(4):\n",
        "    for j in range(2):\n",
        "        essay_set_number+= 1\n",
        "        df[df['essay_set'] == essay_set_number]\\\n",
        "            .groupby('grade')['essay_id']\\\n",
        "            .agg('count')\\\n",
        "            .plot.bar(ax=ax[i, j], rot=0)\n",
        "        ax[i,j].set_title('Essay set %i' % essay_set_number)\n",
        "ax[3,0].locator_params(nbins=10)\n",
        "ax[3,1].locator_params(nbins=10)\n",
        "plt.suptitle('Histograms of essay scores')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfNvNPmFXQIr"
      },
      "source": [
        "# Finetune : BERT, RoBERTa, XLNet, ELECTRA, ALBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFDz58B3iW6D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "\n",
        "class EssayGrader:\n",
        "    def __init__(self, model_name: str, use_cuda: bool = True):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "\n",
        "    def encode_data(self, df):\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        for essay in df['essay']:\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                essay,\n",
        "                add_special_tokens=True,\n",
        "                max_length=512,\n",
        "                padding='max_length', # change pad_to_max_length to padding\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt',\n",
        "                truncation=True # handle sequences longer than model max input length\n",
        "            )\n",
        "\n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(df['grade'].values, dtype=torch.float32)\n",
        "\n",
        "        return input_ids, attention_masks, labels\n",
        "\n",
        "    def prepare_dataloader(self, train_df, test_df, batch_size=8):\n",
        "        train_input_ids, train_attention_masks, train_labels = self.encode_data(train_df)\n",
        "        test_input_ids, test_attention_masks, test_labels = self.encode_data(test_df)\n",
        "\n",
        "        train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "        test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "        self.train_dataloader = DataLoader(\n",
        "            train_data,\n",
        "            sampler=RandomSampler(train_data),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        self.validation_dataloader = DataLoader(\n",
        "            test_data,\n",
        "            sampler=SequentialSampler(test_data),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    def train_model(self, epochs=1):\n",
        "      optimizer = AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
        "      total_steps = len(self.train_dataloader) * epochs\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "      loss_fn = nn.MSELoss()\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          self.model.train()\n",
        "          total_train_loss = 0.0\n",
        "\n",
        "          for step, batch in enumerate(self.train_dataloader):\n",
        "              batch = tuple(t.to(self.device) for t in batch)\n",
        "              input_ids, attention_masks, labels = batch\n",
        "\n",
        "              self.model.zero_grad()\n",
        "              outputs = self.model(\n",
        "                  input_ids=input_ids,\n",
        "                  attention_mask=attention_masks,\n",
        "                  labels=labels\n",
        "              )\n",
        "              logits = outputs.logits\n",
        "              loss = loss_fn(logits.squeeze(), labels)\n",
        "              total_train_loss += loss.item()\n",
        "\n",
        "              loss.backward()\n",
        "              torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "              optimizer.step()\n",
        "              scheduler.step()\n",
        "\n",
        "          avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
        "          print(f\"Epoch {epoch + 1}/{epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def generate_predictions(self, test_df):\n",
        "        self.model.eval()\n",
        "        input_ids, attention_masks, _ = self.encode_data(test_df)\n",
        "        test_data = TensorDataset(input_ids, attention_masks)\n",
        "        test_dataloader = DataLoader(\n",
        "            test_data,\n",
        "            sampler=SequentialSampler(test_data),\n",
        "            batch_size=8\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            input_ids, attention_masks = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=attention_masks\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predicted_grades = logits.squeeze().cpu().numpy().tolist()\n",
        "            predictions.extend(predicted_grades)\n",
        "\n",
        "        df_predictions = test_df.copy()\n",
        "        df_predictions['Predicted Grade'] = predictions\n",
        "\n",
        "        return df_predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO63F0MnjMM2"
      },
      "source": [
        "# Run models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyRiqJdIy8c0"
      },
      "source": [
        "# Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkq2HjaNy3IZ"
      },
      "outputs": [],
      "source": [
        "# Convert grade to float32\n",
        "df['grade'] = df['grade'].astype('float32')\n",
        "\n",
        "# First, we split our dataset (80%/20%)\n",
        "test_size = 0.2\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLyN3LEzu8-B"
      },
      "outputs": [],
      "source": [
        "# train_df = train_df[[\"essay\",\"grade\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAlWolR5jPzM"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxg2246vjM6T"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model_name = 'bert-base-uncased'\n",
        "grader_bert = EssayGrader(model_name)\n",
        "\n",
        "# Prepare dataloader\n",
        "grader_bert.prepare_dataloader(train_df, test_df, batch_size=8)\n",
        "\n",
        "# Train model\n",
        "grader_bert.train_model(epochs=5)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model so that it doesn't have to run again\n",
        "\n",
        "grader_bert.save_model(MODEL_PATH)"
      ],
      "metadata": {
        "id": "HUJPUqX4i23j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-nCtJIkAOi1"
      },
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "predictions_df_bert = grader_bert.generate_predictions(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH12BpFqVGO8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# 'grade' and 'Predicted Grade' are column names containing actual and predicted grades\n",
        "y_true = test_df['grade']\n",
        "y_pred_bert = predictions_df_bert['Predicted Grade'].round()  # round the predictions to make them discrete\n",
        "\n",
        "qwk_bert = cohen_kappa_score(y_true, y_pred_bert, weights='quadratic')\n",
        "\n",
        "print(f\"Quadratic Weighted Kappa (QWK) is: {qwk_bert}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the training and testing data do not overlap\n",
        "assert len(set(train_df.index) & set(test_df.index)) == 0"
      ],
      "metadata": {
        "id": "r1K7IjGlUhVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = MODEL_PATH\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "uREV78aFVBRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_predictions_new(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, attention_masks = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=attention_masks\n",
        "            )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predicted_grades = logits.squeeze().cpu().numpy().tolist()\n",
        "        predictions.extend(predicted_grades)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Prepare your DataLoader for the test data\n",
        "# You can use the encode_data and DataLoader initialization logic from the `EssayGrader` class here\n",
        "test_input_ids, test_attention_masks, _ = grader_bert.encode_data(test_df.head())\n",
        "test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
        "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=8)\n",
        "\n",
        "# Generate predictions\n",
        "predictions = generate_predictions_new(model, test_dataloader)\n",
        "\n",
        "# Add the predictions to the DataFrame\n",
        "temp = test_df.head().copy()\n",
        "temp['Predicted'] = predictions"
      ],
      "metadata": {
        "id": "rZS2QP0qVHhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dg-h-EcejolJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp"
      ],
      "metadata": {
        "id": "EjRjcYAjao6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1O3SlQ-jQ0O"
      },
      "source": [
        "## XLNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "JEEdOc7BGWFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUR8HV5eVndc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import sentencepiece\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "class EssayGrader:\n",
        "    def __init__(self, model_name: str, use_cuda: bool = True):\n",
        "        self.tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
        "        self.model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "\n",
        "    def encode_data(self, df):\n",
        "          input_ids = []\n",
        "          attention_masks = []\n",
        "\n",
        "          for essay in df['essay']:\n",
        "              encoded_dict = self.tokenizer.encode_plus(\n",
        "                  essay,\n",
        "                  add_special_tokens=True,\n",
        "                  max_length=512,\n",
        "                  padding='max_length', # change pad_to_max_length to padding\n",
        "                  return_attention_mask=True,\n",
        "                  return_tensors='pt',\n",
        "                  truncation=True # handle sequences longer than model max input length\n",
        "              )\n",
        "\n",
        "              input_ids.append(encoded_dict['input_ids'])\n",
        "              attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "          input_ids = torch.cat(input_ids, dim=0)\n",
        "          attention_masks = torch.cat(attention_masks, dim=0)\n",
        "          labels = torch.tensor(df['grade'].values, dtype=torch.float32)\n",
        "\n",
        "          return input_ids, attention_masks, labels\n",
        "\n",
        "    def prepare_dataloader(self, train_df, test_df, batch_size=8):\n",
        "          train_input_ids, train_attention_masks, train_labels = self.encode_data(train_df)\n",
        "          test_input_ids, test_attention_masks, test_labels = self.encode_data(test_df)\n",
        "\n",
        "          train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "          test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "          self.train_dataloader = DataLoader(\n",
        "              train_data,\n",
        "              sampler=RandomSampler(train_data),\n",
        "              batch_size=batch_size\n",
        "          )\n",
        "\n",
        "          self.validation_dataloader = DataLoader(\n",
        "              test_data,\n",
        "              sampler=SequentialSampler(test_data),\n",
        "              batch_size=batch_size\n",
        "          )\n",
        "\n",
        "    def train_model(self, epochs=1):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                input_ids, attention_masks, labels = batch\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_masks,\n",
        "                    labels=labels\n",
        "                )\n",
        "                logits = outputs.logits\n",
        "                loss = loss_fn(logits.squeeze(), labels)\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def generate_predictions(self, test_df):\n",
        "        self.model.eval()\n",
        "        input_ids, attention_masks, _ = self.encode_data(test_df)\n",
        "        test_data = TensorDataset(input_ids, attention_masks)\n",
        "        test_dataloader = DataLoader(\n",
        "            test_data,\n",
        "            sampler=SequentialSampler(test_data),\n",
        "            batch_size=8\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            input_ids, attention_masks = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=attention_masks\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predicted_grades = logits.squeeze().cpu().numpy().tolist()\n",
        "            predictions.extend(predicted_grades)\n",
        "\n",
        "        df_predictions = test_df.copy()\n",
        "        df_predictions['Predicted Grade'] = predictions\n",
        "\n",
        "        return df_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "DATA_PATH = \"drive/MyDrive//training_set_rel3.tsv\"\n",
        "df = pd.read_csv(DATA_PATH, sep='\\t', encoding='ISO-8859-1')\n",
        "df.rename(columns={'essay': 'essay','domain1_score' : 'grade'}, inplace=True)\n",
        "df = df[[\"essay_id\",\"essay_set\",\"essay\",\"grade\"]]\n",
        "\n",
        "# Convert grade to float32\n",
        "df['grade'] = df['grade'].astype('float32')\n",
        "\n",
        "# First, we split our dataset (80%/20%)\n",
        "test_size = 0.2\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)"
      ],
      "metadata": {
        "id": "ZuX0Cuz6HHLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7aycnY3jis5"
      },
      "outputs": [],
      "source": [
        "# Usage\n",
        "\n",
        "model_name = 'xlnet-base-cased'\n",
        "grader_xlnet = EssayGrader(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grader_xlnet.prepare_dataloader(train_df, test_df, batch_size=8)\n",
        "grader_xlnet.train_model(epochs=5)\n",
        "predictions_df_xlnet = grader_xlnet.generate_predictions(test_df)\n",
        "predictions_df_xlnet"
      ],
      "metadata": {
        "id": "KA0Xz2Uk5kST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# 'grade' and 'Predicted Grade' are column names containing actual and predicted grades\n",
        "y_true = test_df['grade']\n",
        "y_pred_xlnet = predictions_df_xlnet['Predicted Grade'].round()  # round the predictions to make them discrete\n",
        "\n",
        "qwk_xlnet = cohen_kappa_score(y_true, y_pred_xlnet, weights='quadratic')\n",
        "\n",
        "print(f\"Quadratic Weighted Kappa (QWK) is: {qwk_xlnet}\")"
      ],
      "metadata": {
        "id": "9oTP2jmPDlv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader_xlnet.save_model(XLNET_MODEL_PATH)"
      ],
      "metadata": {
        "id": "R_c0-eRxFeso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55MxFYTDjQ2C"
      },
      "source": [
        "## ELECTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJcsCpHmWqIe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "\n",
        "class EssayGrader:\n",
        "    def __init__(self, model_name: str, use_cuda: bool = True):\n",
        "        self.tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
        "        self.model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "\n",
        "    def encode_data(self, df):\n",
        "          input_ids = []\n",
        "          attention_masks = []\n",
        "\n",
        "          for essay in df['essay']:\n",
        "              encoded_dict = self.tokenizer.encode_plus(\n",
        "                  essay,\n",
        "                  add_special_tokens=True,\n",
        "                  max_length=512,\n",
        "                  padding='max_length', # change pad_to_max_length to padding\n",
        "                  return_attention_mask=True,\n",
        "                  return_tensors='pt',\n",
        "                  truncation=True # handle sequences longer than model max input length\n",
        "              )\n",
        "\n",
        "              input_ids.append(encoded_dict['input_ids'])\n",
        "              attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "          input_ids = torch.cat(input_ids, dim=0)\n",
        "          attention_masks = torch.cat(attention_masks, dim=0)\n",
        "          labels = torch.tensor(df['grade'].values, dtype=torch.float32)\n",
        "\n",
        "          return input_ids, attention_masks, labels\n",
        "\n",
        "    def prepare_dataloader(self, train_df, test_df, batch_size=8):\n",
        "          train_input_ids, train_attention_masks, train_labels = self.encode_data(train_df)\n",
        "          test_input_ids, test_attention_masks, test_labels = self.encode_data(test_df)\n",
        "\n",
        "          train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "          test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "          self.train_dataloader = DataLoader(\n",
        "              train_data,\n",
        "              sampler=RandomSampler(train_data),\n",
        "              batch_size=batch_size\n",
        "          )\n",
        "\n",
        "          self.validation_dataloader = DataLoader(\n",
        "              test_data,\n",
        "              sampler=SequentialSampler(test_data),\n",
        "              batch_size=batch_size\n",
        "          )\n",
        "\n",
        "    def train_model(self, epochs=1):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                input_ids, attention_masks, labels = batch\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_masks,\n",
        "                    labels=labels\n",
        "                )\n",
        "                logits = outputs.logits\n",
        "                loss = loss_fn(logits.squeeze(), labels)\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def generate_predictions(self, test_df):\n",
        "        self.model.eval()\n",
        "        input_ids, attention_masks, _ = self.encode_data(test_df)\n",
        "        test_data = TensorDataset(input_ids, attention_masks)\n",
        "        test_dataloader = DataLoader(\n",
        "            test_data,\n",
        "            sampler=SequentialSampler(test_data),\n",
        "            batch_size=8\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            input_ids, attention_masks = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=attention_masks\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predicted_grades = logits.squeeze().cpu().numpy().tolist()\n",
        "            predictions.extend(predicted_grades)\n",
        "\n",
        "        df_predictions = test_df.copy()\n",
        "        df_predictions['Predicted Grade'] = predictions\n",
        "\n",
        "        return df_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBlArKFojjDB"
      },
      "outputs": [],
      "source": [
        "model_name = 'google/electra-small-discriminator'\n",
        "grader_electra = EssayGrader(model_name)\n",
        "grader_electra.prepare_dataloader(train_df, test_df, batch_size=8)\n",
        "grader_electra.train_model(epochs=5)\n",
        "predictions_df_electra = grader_electra.generate_predictions(test_df)\n",
        "predictions_df_electra"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'grade' and 'Predicted Grade' are column names containing actual and predicted grades\n",
        "y_true = test_df['grade']\n",
        "y_pred_electra = predictions_df_electra['Predicted Grade'].round()  # round the predictions to make them discrete\n",
        "\n",
        "qwk_electra = cohen_kappa_score(y_true, y_pred_electra, weights='quadratic')\n",
        "\n",
        "print(f\"Quadratic Weighted Kappa (QWK) is: {qwk_electra}\")"
      ],
      "metadata": {
        "id": "rf3vaDHLG8Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKQ777BPjQ33"
      },
      "source": [
        "## ALBERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "0hqovb7zr5Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "DATA_PATH = \"drive/MyDrive//training_set_rel3.tsv\"\n",
        "df = pd.read_csv(DATA_PATH, sep='\\t', encoding='ISO-8859-1')\n",
        "df.rename(columns={'essay': 'essay','domain1_score' : 'grade'}, inplace=True)\n",
        "df = df[[\"essay_id\",\"essay_set\",\"essay\",\"grade\"]]\n",
        "\n",
        "# Convert grade to float32\n",
        "df['grade'] = df['grade'].astype('float32')\n",
        "\n",
        "# First, we split our dataset (80%/20%)\n",
        "test_size = 0.2\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)"
      ],
      "metadata": {
        "id": "yKzZoFM9r9io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIT47ps6W41M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "\n",
        "class EssayGrader:\n",
        "    def __init__(self, model_name: str, use_cuda: bool = True):\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "        self.model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "\n",
        "    def encode_data(self, df):\n",
        "          input_ids = []\n",
        "          attention_masks = []\n",
        "\n",
        "          for essay in df['essay']:\n",
        "              encoded_dict = self.tokenizer.encode_plus(\n",
        "                  essay,\n",
        "                  add_special_tokens=True,\n",
        "                  max_length=512,\n",
        "                  padding='max_length', # change pad_to_max_length to padding\n",
        "                  return_attention_mask=True,\n",
        "                  return_tensors='pt',\n",
        "                  truncation=True # handle sequences longer than model max input length\n",
        "              )\n",
        "\n",
        "              input_ids.append(encoded_dict['input_ids'])\n",
        "              attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "          input_ids = torch.cat(input_ids, dim=0)\n",
        "          attention_masks = torch.cat(attention_masks, dim=0)\n",
        "          labels = torch.tensor(df['grade'].values, dtype=torch.float32)\n",
        "\n",
        "          return input_ids, attention_masks, labels\n",
        "\n",
        "    def prepare_dataloader(self, train_df, test_df, batch_size=8):\n",
        "          train_input_ids, train_attention_masks, train_labels = self.encode_data(train_df)\n",
        "          test_input_ids, test_attention_masks, test_labels = self.encode_data(test_df)\n",
        "\n",
        "          train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "          test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "          self.train_dataloader = DataLoader(\n",
        "              train_data,\n",
        "              sampler=RandomSampler(train_data),\n",
        "              batch_size=batch_size\n",
        "          )\n",
        "\n",
        "          self.validation_dataloader = DataLoader(\n",
        "              test_data,\n",
        "              sampler=SequentialSampler(test_data),\n",
        "              batch_size=batch_size\n",
        "          )\n",
        "\n",
        "    def train_model(self, epochs=1):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                input_ids, attention_masks, labels = batch\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_masks,\n",
        "                    labels=labels\n",
        "                )\n",
        "                logits = outputs.logits\n",
        "                loss = loss_fn(logits.squeeze(), labels)\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def generate_predictions(self, test_df):\n",
        "        self.model.eval()\n",
        "        input_ids, attention_masks, _ = self.encode_data(test_df)\n",
        "        test_data = TensorDataset(input_ids, attention_masks)\n",
        "        test_dataloader = DataLoader(\n",
        "            test_data,\n",
        "            sampler=SequentialSampler(test_data),\n",
        "            batch_size=8\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            input_ids, attention_masks = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=attention_masks\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predicted_grades = logits.squeeze().cpu().numpy().tolist()\n",
        "            predictions.extend(predicted_grades)\n",
        "\n",
        "        df_predictions = test_df.copy()\n",
        "        df_predictions['Predicted Grade'] = predictions\n",
        "\n",
        "        return df_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob9MjaQUjjbR"
      },
      "outputs": [],
      "source": [
        "model_name = 'albert-base-v2'\n",
        "grader_albert = EssayGrader(model_name)\n",
        "grader_albert.prepare_dataloader(train_df, test_df, batch_size=8)\n",
        "grader_albert.train_model(epochs=10)\n",
        "predictions_df_albert = grader_albert.generate_predictions(test_df)\n",
        "predictions_df_albert"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'grade' and 'Predicted Grade' are column names containing actual and predicted grades\n",
        "y_true = test_df['grade']\n",
        "y_pred_albert = predictions_df_albert['Predicted Grade'].round()  # round the predictions to make them discrete\n",
        "\n",
        "qwk_albert = cohen_kappa_score(y_true, y_pred_albert, weights='quadratic')\n",
        "\n",
        "print(f\"Quadratic Weighted Kappa (QWK) is: {qwk_albert}\")"
      ],
      "metadata": {
        "id": "b8NKjqWuwcFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3k-7PwKXzla"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Saving BERT model\n",
        "model_path = \"/content/gdrive/MyDrive/bert_model.pth\"\n",
        "torch.save(grader_bert.model.state_dict(), model_path)\n",
        "\n",
        "# Saving XLNet model\n",
        "model_path = \"/content/gdrive/MyDrive/xlnet_model.pth\"\n",
        "torch.save(grader_xlnet.model.state_dict(), model_path)\n",
        "\n",
        "# Saving Electra model\n",
        "model_path = \"/content/gdrive/MyDrive/electra_model.pth\"\n",
        "torch.save(grader_electra.model.state_dict(), model_path)\n",
        "\n",
        "# Saving Albert model\n",
        "model_path = \"/content/gdrive/MyDrive/albert_model.pth\"\n",
        "torch.save(grader_albert.model.state_dict(), model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b16Lj-uXYQLi"
      },
      "outputs": [],
      "source": [
        "# Loading BERT model\n",
        "model_path = \"/content/gdrive/MyDrive/bert_model.pth\"\n",
        "bert_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "bert_model.load_state_dict(torch.load(model_path))\n",
        "bert_model.eval()\n",
        "\n",
        "# Loading XLNet model\n",
        "model_path = \"/content/gdrive/MyDrive/xlnet_model.pth\"\n",
        "xlnet_model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "xlnet_model.load_state_dict(torch.load(model_path))\n",
        "xlnet_model.eval()\n",
        "\n",
        "# Loading Electra model\n",
        "model_path = \"/content/gdrive/MyDrive/electra_model.pth\"\n",
        "electra_model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "electra_model.load_state_dict(torch.load(model_path))\n",
        "electra_model.eval()\n",
        "\n",
        "# Loading Albert model\n",
        "model_path = \"/content/gdrive/MyDrive/albert_model.pth\"\n",
        "albert_model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "albert_model.load_state_dict(torch.load(model_path))\n",
        "albert_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8Z-3GXy1jZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQxZtFcHkvh0"
      },
      "source": [
        "# GPT-3 few-shot learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px4heWQMkzQo"
      },
      "outputs": [],
      "source": [
        "\n",
        "openai.api_key = ''\n",
        "\n",
        "def grade_essay_one_gpt3(essay, essay_set):\n",
        "    prompt = f\"\"\"\n",
        "    Context: 'You are an AI model with extensive knowledge in language and writing styles. You're provided with a pool of essays, each corresponding to a specific essay set with a unique grading rubric.\n",
        "    The grading rubrics:\n",
        "    Essay Set 1 Rubric:\n",
        "          Rubric range: 1-6\n",
        "          Score Point 1: An undeveloped response that may take a position but offers no more than very minimal support. Typical elements:\n",
        "          Contains few or vague details, ss awkward and fragmented, may be difficult to read and understand, may show no awareness of audience.\n",
        "          Score Point 2: An under-developed response that may or may not take a position. Typical elements:\n",
        "          Contains only general reasons with unelaborated and/or list-like details, shows little or no evidence of organization, may be awkward and confused or simplistic, may show little awareness of audience.\n",
        "          Score Point 3: A minimally-developed response that may take a position, but with inadequate support and details. Typical elements:\n",
        "          Has reasons with minimal elaboration and more general than specific details, shows some organization, may be awkward in parts with few transitions, shows some awareness of audience.\n",
        "          Score Point 4: A somewhat-developed response that takes a position and provides adequate support. Typical elements:\n",
        "          Has adequately elaborated reasons with a mix of general and specific details, shows satisfactory organization, may be somewhat fluent with some transitional language, shows adequate awareness of audience.\n",
        "          Score Point 5: A developed response that takes a clear position and provides reasonably persuasive support. Typical elements:\n",
        "          Has moderately well elaborated reasons with mostly specific details, exhibits generally strong organization, may be moderately fluent with transitional language throughout, may show a consistent awareness of audience.\n",
        "          Score Point 6: A well-developed response that takes a clear and thoughtful position and provides persuasive support. Typical elements:\n",
        "          Has fully elaborated reasons with specific details, exhibits strong organization, is fluent and uses sophisticated transitional language, may show a heightened awareness of audience.\n",
        "\n",
        "\n",
        "    Instruction:\n",
        "    Given the provided essay and its essay set number, apply the grading rubric associated with that essay set and provide a grade as an integer. Important: You should produce a number without any text!\n",
        "\n",
        "    YOUR ANSWER SHOULD ONLY BE A NUMBER. JUST A NUMBER, NO OTHER WORDS, NOTHING ELSE.\n",
        "\n",
        "    Essay set: \"{essay_set}\"\n",
        "\n",
        "    {essay}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=10\n",
        "    )\n",
        "\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output # Convert the grade from string to integer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_essay_two_gpt3(essay, essay_set):\n",
        "    prompt = f\"\"\"\n",
        "    Context: 'You are an AI model with extensive knowledge in language and writing styles. You're provided with a pool of essays, each corresponding to a specific essay set with a unique grading rubric.\n",
        "    The grading rubrics:\n",
        "Essay Set 2 Rubric:\n",
        "          We add the scores from the following two domains:\n",
        "          Rubric range domain 1: 1-6\n",
        "          Score Point 6: A Score Point 6 paper is rare. It fully accomplishes the task in a thorough and insightful manner and has a distinctive quality that sets it apart as an outstanding performance.\n",
        "          Score Point 5: A Score Point 5 paper represents a solid performance. It fully accomplishes the task, but lacks the overall level of sophistication and consistency of a Score Point 6 paper.\n",
        "          Score Point 4: A Score Point 4 paper represents a good performance. It accomplishes the task, but generally needs to exhibit more development, better organization, or a more sophisticated writing style to receive a higher score.\n",
        "          Score Point 3: A Score Point 3 paper represents a performance that minimally accomplishes the task. Some elements of development, organization, and writing style are weak.\n",
        "          Score Point 2: A Score Point 2 paper represents a performance that only partially accomplishes the task. Some responses may exhibit difficulty maintaining a focus. Others may be too brief to provide sufficient development of the topic or evidence of adequate organizational or writing style.\n",
        "          Score Point 1: A Score Point 1 paper represents a performance that fails to accomplish the task. It exhibits considerable difficulty in areas of development, organization, and writing style. The writing is generally either very brief or rambling and repetitive, sometimes resulting in a response that may be difficult to read or comprehend.\n",
        "          Rubric range domain 2: 1-4\n",
        "          Score 4: Does the writing sample exhibit a superior command of language skills?\n",
        "          A Score Point 4 paper exhibits a superior command of written English language conventions. The paper provides evidence that the student has a thorough control of the concepts outlined in the Indiana Academic Standards associated with the student’s grade level. In a Score Point 4 paper, there are no errors that impair the flow of communication. Errors are generally of the first-draft variety or occur when the student attempts sophisticated sentence construction.\n",
        "          Score 3: Does the writing sample exhibit a good control of language skills?\n",
        "          In a Score Point 3 paper, errors are occasional and are often of the first-draft variety; they have a minor impact on the flow of communication.\n",
        "          Score 2: Does the writing sample exhibit a fair control of language skills?\n",
        "          In a Score Point 2 paper, errors are typically frequent and may occasionally impede the flow of communication.\n",
        "          Score 1: Does the writing sample exhibit a minimal or less than minimal control of language skills?\n",
        "          In a Score Point 1 paper, errors are serious and numerous. The reader may need to stop and reread part of the sample and may struggle to discern the writer’s meaning.\n",
        "    Essay Set 3 and 4 Rubric:\n",
        "\n",
        "    Instruction:\n",
        "    Given the provided essay and its essay set number, apply the grading rubric associated with that essay set and provide a grade as an integer. Important: You should produce a number without any text!\n",
        "\n",
        "    YOUR ANSWER SHOULD ONLY BE A NUMBER. JUST A NUMBER, NO OTHER WORDS, NOTHING ELSE.\n",
        "\n",
        "    Essay set: \"{essay_set}\"\n",
        "\n",
        "    {essay}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=10\n",
        "    )\n",
        "\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output # Convert the grade from string to integer"
      ],
      "metadata": {
        "id": "fM1VG-vcS6bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_essay_three_four_gpt3(essay, essay_set):\n",
        "    prompt = f\"\"\"\n",
        "    Context: 'You are an AI model with extensive knowledge in language and writing styles. You're provided with a pool of essays, each corresponding to a specific essay set with a unique grading rubric.\n",
        "    The grading rubrics:\n",
        "\n",
        "    Essay Set 3 and 4 Rubric:\n",
        "    Rubric range: 0-3\n",
        "          Score 0: The response is completely irrelevant or incorrect, or there is no response.\n",
        "          Score 1: The response shows evidence of a minimal understanding of the text. Typical elements:\n",
        "          May show evidence that some meaning has been derived from the text, may indicate a misreading of the text or the question, may lack information or explanation to support an understanding of the text in relation to the question\n",
        "          Score 2: The response demonstrates a partial or literal understanding of the text. Typical elements:\n",
        "          Addresses the demands of the question, although may not develop all parts equally, uses some expressed or implied information from the text to demonstrate understanding, may not fully connect the support to a conclusion or assertion made about the text(s)\n",
        "          Score 3: The response demonstrates an understanding of the complexities of the text. Typical elements:\n",
        "          Addresses the demands of the question, uses expressed and implied information from the text, clarifies and extends understanding beyond the literal\n",
        "\n",
        "    Instruction:\n",
        "    Given the provided essay and its essay set number, apply the grading rubric associated with that essay set and provide a grade as an integer. Important: You should produce a number without any text!\n",
        "\n",
        "    YOUR ANSWER SHOULD ONLY BE A NUMBER. JUST A NUMBER, NO OTHER WORDS, NOTHING ELSE.\n",
        "\n",
        "    Essay set: \"{essay_set}\"\n",
        "\n",
        "    {essay}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=10\n",
        "    )\n",
        "\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output # Convert the grade from string to integer"
      ],
      "metadata": {
        "id": "SCD8b06oS6jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_essay_five_six_gpt3(essay, essay_set):\n",
        "    prompt = f\"\"\"\n",
        "    Context: 'You are an AI model with extensive knowledge in language and writing styles. You're provided with a pool of essays, each corresponding to a specific essay set with a unique grading rubric.\n",
        "    The grading rubrics:\n",
        "\n",
        "\n",
        "    Essay Set 5 and 6 Rubric:\n",
        "          Rubric range: 0-4\n",
        "          Score Point 0: The response is incorrect or irrelevant or contains insufficient information to demonstrate comprehension.\n",
        "          Score Point 1: The response is a minimal description of the mood created by the author. The response includes little or no information from the memoir and may include misinterpretations or the response relates minimally to the task.\n",
        "          Score Point 2: The response is a partial description of the mood created by the author. The response includes limited information from the memoir and may include misinterpretations.\n",
        "          Score Point 3: The response is a mostly clear, complete, and accurate description of the mood created by the author. The response includes relevant but often general information from the memoir.\n",
        "          Score Point 4: The response is a clear, complete, and accurate description of the mood created by the author. The response includes relevant and specific information from the memoir.\n",
        "\n",
        "    Instruction:\n",
        "    Given the provided essay and its essay set number, apply the grading rubric associated with that essay set and provide a grade as an integer. Important: You should produce a number without any text!\n",
        "\n",
        "    YOUR ANSWER SHOULD ONLY BE A NUMBER. JUST A NUMBER, NO OTHER WORDS, NOTHING ELSE.\n",
        "\n",
        "    Essay set: \"{essay_set}\"\n",
        "\n",
        "    {essay}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=10\n",
        "    )\n",
        "\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output # Convert the grade from string to integer"
      ],
      "metadata": {
        "id": "jqOQOVrCS6rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_essay_seven_gpt3(essay, essay_set):\n",
        "    prompt = f\"\"\"\n",
        "    Context: 'You are an AI model with extensive knowledge in language and writing styles. You're provided with a pool of essays, each corresponding to a specific essay set with a unique grading rubric.\n",
        "    The grading rubrics:\n",
        "\n",
        "\n",
        "    Essay Set 7 Rubric:\n",
        "          Rubric range: 0-15\n",
        "          We add the scores from each of following four traits:\n",
        "          Ideas\n",
        "          Score 6: Tells a story with ideas that are clearly focused on the topic and are thoroughly developed with specific, relevant details.\n",
        "          Score 4: Tells a story with ideas that are somewhat focused on the topic and are developed with a mix of specific and/or general details.\n",
        "          Score 2: Tells a story with ideas that are minimally focused on the topic and developed with limited and/or general details.\n",
        "          Score 0: Ideas are not focused on the task and/or are undeveloped.\n",
        "          Organization\n",
        "          Score 3: Organization and connections between ideas and/or events are clear and logically sequenced.\n",
        "          Score 2: Organization and connections between ideas and/or events are logically sequenced.\n",
        "          Score 1: Organization and connections between ideas and/or events are weak.\n",
        "          Score 0: No organization evident.\n",
        "          Style\n",
        "          Score 3: Command of language, including effective and compelling word choice and varied sentence structure, clearly supports the writer's purpose and audience.\n",
        "          Score 2: Adequate command of language, including effective word choice and clear sentences, supports the writer's purpose and audience.\n",
        "          Score 1: Limited use of language, including lack of variety in word choice and sentences, may hinder support for the writer's purpose and audience.\n",
        "          Score 0: Ineffective use of language for the writer's purpose and audience.\n",
        "          Conventions\n",
        "          Score 3: Consistent, appropriate use of conventions of Standard English for grammar, usage, spelling, capitalization, and punctuation for the grade level.\n",
        "          Score 2: Adequate use of conventions of Standard English for grammar, usage, spelling, capitalization, and punctuation for the grade level.\n",
        "          Score 1: Limited use of conventions of Standard English for grammar, usage, spelling, capitalization, and punctuation for the grade level.\n",
        "          Score 0: Ineffective use of conventions of Standard English for grammar, usage, spelling, capitalization, and punctuation.\n",
        "\n",
        "    Instruction:\n",
        "    Given the provided essay and its essay set number, apply the grading rubric associated with that essay set and provide a grade as an integer. Important: You should produce a number without any text!\n",
        "\n",
        "    YOUR ANSWER SHOULD ONLY BE A NUMBER. JUST A NUMBER, NO OTHER WORDS, NOTHING ELSE.\n",
        "\n",
        "    Essay set: \"{essay_set}\"\n",
        "\n",
        "    {essay}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=10\n",
        "    )\n",
        "\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output # Convert the grade from string to integer"
      ],
      "metadata": {
        "id": "K50yvr6fS6ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_essay_eight_gpt3(essay, essay_set):\n",
        "    prompt = f\"\"\"\n",
        "    Context: 'You are an AI model with extensive knowledge in language and writing styles. You're provided with a pool of essays, each corresponding to a specific essay set with a unique grading rubric.\n",
        "    The grading rubrics:\n",
        "\n",
        "\n",
        "    Essay Set 8 Rubric:\n",
        "          Rubric Guidelines\n",
        "          A rating of 1-6 on the following six traits:\n",
        "          Ideas and Content\n",
        "          Score 12: The writing is exceptionally clear, focused, and interesting. It holds the reader’s attention throughout. Main ideas stand out and are developed by strong support and rich details suitable to audience and purpose. The writing is characterized by\n",
        "          Score 10: The writing is clear, focused and interesting. It holds the reader’s attention. Main ideas stand out and are developed by supporting details suitable to audience and purpose. The writing is characterized by\n",
        "          Score 8: The writing is clear and focused. The reader can easily understand the main ideas. Support is present, although it may be limited or rather general. The writing is characterized by\n",
        "          Score 6: The reader can understand the main ideas, although they may be overly broad or simplistic, and the results may not be effective. Supporting detail is often limited, insubstantial, overly general, or occasionally slightly off-topic. The writing is characterized by\n",
        "          Score 4: Main ideas and purpose are somewhat unclear or development is attempted but minimal. The writing is characterized by\n",
        "          Score 2: The writing lacks a central idea or purpose.\n",
        "          Organization\n",
        "          Score 12: The organization enhances the central idea(s) and its development. The order and structure are compelling and move the reader through the text easily.\n",
        "          Score 10: The organization enhances the central idea(s) and its development. The order and structure are strong and move the reader through the text.\n",
        "          Score 8: Organization is clear and coherent. Order and structure are present, but may seem formulaic. The writing is characterized by\n",
        "          Score 6: An attempt has been made to organize the writing; however, the overall structure is inconsistent or skeletal.\n",
        "          Score 4: The writing lacks a clear organizational structure. An occasional organizational device is discernible; however, the writing is either difficult to follow and the reader has to reread substantial portions, or the piece is simply too short to demonstrate organizational skills.\n",
        "          Score 2: The writing lacks coherence; organization seems haphazard and disjointed. Even after rereading, the reader remains confused.\n",
        "          Sentence Fluency\n",
        "          Score 12: The writing has an effective flow and rhythm. Sentences show a high degree of craftsmanship, with consistently strong and varied structure that makes expressive oral reading easy and enjoyable.\n",
        "          Score 10: The writing has an easy flow and rhythm. Sentences are carefully crafted, with strong and varied structure that makes expressive oral reading easy and enjoyable.\n",
        "          Score 8: The writing flows; however, connections between phrases or sentences may be less than fluid. Sentence patterns are somewhat varied, contributing to ease in oral reading.\n",
        "          Score 6: The writing tends to be mechanical rather than fluid. Occasional awkward constructions may force the reader to slow down or reread.\n",
        "          Score 4: The writing tends to be either choppy or rambling. Awkward constructions often force the reader to slow down or reread.\n",
        "          Score 2: The writing is difficult to follow or to read aloud. Sentences tend to be incomplete, rambling, or very awkward.\n",
        "          Conventions\n",
        "          Score 24: The writing demonstrates exceptionally strong control of standard writing conventions (e.g., punctuation, spelling, capitalization, grammar and usage) and uses them effectively to enhance communication. Errors are so few and so minor that the reader can easily skim right over them unless specifically searching for them.\n",
        "          Score 20: The writing demonstrates strong control of standard writing conventions (e.g., punctuation, spelling, capitalization, grammar and usage) and uses them effectively to enhance communication. Errors are few and minor. Conventions support readability.\n",
        "          Score 16: The writing demonstrates control of standard writing conventions (e.g., punctuation, spelling, capitalization, grammar and usage). Significant errors do not occur frequently. Minor errors, while perhaps noticeable, do not impede readability.\n",
        "          Score 12: The writing demonstrates limited control of standard writing conventions (e.g., punctuation, spelling, capitalization, grammar and usage). Errors begin to impede readability.\n",
        "          Score 8: The writing demonstrates little control of standard writing conventions. Frequent, significant errors impede readability.\n",
        "          Score 4: Numerous errors in usage, spelling, capitalization, and punctuation repeatedly distract the reader and make the text difficult to read. In fact, the severity and frequency of errors are so overwhelming that the reader finds it difficult to focus on the message and must reread for meaning.\n",
        "\n",
        "    Instruction:\n",
        "    Given the provided essay and its essay set number, apply the grading rubric associated with that essay set and provide a grade as an integer. Important: You should produce a number without any text!\n",
        "\n",
        "    YOUR ANSWER SHOULD ONLY BE A NUMBER. JUST A NUMBER, NO OTHER WORDS, NOTHING ELSE.\n",
        "\n",
        "    Essay set: \"{essay_set}\"\n",
        "\n",
        "    {essay}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=10\n",
        "    )\n",
        "\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output # Convert the grade from string to integer"
      ],
      "metadata": {
        "id": "ws_q1MpGS64a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbojm452ZF12"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_gpt3_predictions(df):\n",
        "    gpt3_predictions = []\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        essay = row['essay']\n",
        "        essay_set = row['essay_set']\n",
        "        if essay_set == 1:\n",
        "          grade = grade_essay_one_gpt3(essay, essay_set)\n",
        "        if essay_set == 2:\n",
        "          grade = grade_essay_two_gpt3(essay, essay_set)\n",
        "        if essay_set == 3 or essay_set == 4 :\n",
        "          grade = grade_essay_three_four_gpt3(essay, essay_set)\n",
        "        if essay_set == 5 or essay_set == 6:\n",
        "          grade = grade_essay_five_six_gpt3(essay, essay_set)\n",
        "        if essay_set == 7:\n",
        "          grade = grade_essay_seven_gpt3(essay, essay_set)\n",
        "        if essay_set == 8:\n",
        "          grade = grade_essay_eight_gpt3(essay, essay_set)\n",
        "\n",
        "        gpt3_predictions.append(grade)\n",
        "    return gpt3_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSQORYcZZNff"
      },
      "outputs": [],
      "source": [
        "gpt3_predictions = generate_gpt3_predictions(smaller_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smaller_dataset"
      ],
      "metadata": {
        "id": "W7fjVW8RidzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smaller_dataset.to_csv('smaller_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "y2pwavjFiqiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"list.txt\", \"w\") as file:\n",
        "    for item in gpt3_predictions:\n",
        "        file.write(str(item) + \"\\n\")"
      ],
      "metadata": {
        "id": "0kyX0IkHchw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Specify the column for which you want to maintain the distribution\n",
        "target_column = \"essay_set\"\n",
        "\n",
        "# Set the desired size of the smaller dataset\n",
        "desired_dataset_size = 1000\n",
        "\n",
        "# Perform stratified sampling to create the smaller dataset\n",
        "smaller_dataset = train_df.groupby(target_column, group_keys=False).apply(\n",
        "    lambda x: x.sample(int(desired_dataset_size / len(train_df) * len(x)))\n",
        ")\n",
        "\n",
        "# Reset the index of the smaller dataset\n",
        "smaller_dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Optional: Save the smaller dataset to a new CSV file\n",
        "smaller_dataset.to_csv(\"smaller_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "iaNfjywxan2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(gpt3_predictions)"
      ],
      "metadata": {
        "id": "novlrK5CMYTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_predictions[:5]"
      ],
      "metadata": {
        "id": "bqNOPCW0qi8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zr9VvP12NKVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true.head()"
      ],
      "metadata": {
        "id": "Ga7_i4vkqqRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iSbm34cBqp62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'grade' and 'Predicted Grade' are column names containing actual and predicted grades\n",
        "y_true = test_df['grade']\n",
        "y_pred_gpt3 = gpt3_predictions  # round the predictions to make them discrete\n",
        "\n",
        "qwk_albert = cohen_kappa_score(y_true, y_pred_gpt3, weights='quadratic')\n",
        "\n",
        "print(f\"Quadratic Weighted Kappa (QWK) is: {qwk_albert}\")"
      ],
      "metadata": {
        "id": "wXqx39-1iXBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFh7UZrhkJyI"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwSW6EIvkKi6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Assuming the grades from your models are contained in a list of lists where\n",
        "# each list contains the grades from a single model\n",
        "grades = [[...], [...], [...], [...], [...]]  # BERT, ALBERT, ELECTRA, XLNet, GPT-3\n",
        "\n",
        "# Convert grades to features and targets\n",
        "X = list(zip(*grades))  # features are the grades from all models\n",
        "y = [...]  # target is the actual grades\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "regressor = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "regressor.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVBitsLvZQsR"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"path/to/spiece_model\", \"path/to/special_tokens_map.json\")\n",
        "\n",
        "# Step 2: Load the model architecture\n",
        "config = BertConfig.from_json_file(\"path/to/config.json\")\n",
        "model = BertModel(config)\n",
        "\n",
        "# Step 3: Load the model weights\n",
        "model.load_state_dict(torch.load(\"path/to/pytorch_model.bin\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraTokenizer, ElectraConfig, ElectraModel\n",
        "\n",
        "# Step 1: Load the tokenizer\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\"/content/drive/MyDrive/model/electra_/vocab.txt\", \"/content/drive/MyDrive/model/electra_/special_tokens_map.json\")\n",
        "\n",
        "# Step 2: Load the model architecture\n",
        "config = ElectraConfig.from_json_file(\"/content/drive/MyDrive/model/electra_/config.json\")\n",
        "model_electra = ElectraModel(config)\n",
        "\n",
        "# Step 3: Load the model weights\n",
        "model_electra.load_state_dict(torch.load(\"/content/drive/MyDrive/model/electra_/pytorch_model.bin\"))\n"
      ],
      "metadata": {
        "id": "bPETNKkmfhR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AlbertTokenizer, AlbertConfig, XLNetModel\n",
        "\n",
        "# Step 1: Load the tokenizer\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"/content/drive/MyDrive/model/albert/spiece.model\", \"/content/drive/MyDrive/model/albert/special_tokens_map.json\")\n",
        "# Step 2: Load the model architecture\n",
        "config = AlbertConfig.from_json_file(\"/content/drive/MyDrive/model/albert/config.json\")\n",
        "model_albert = AlbertModel(config)\n",
        "\n",
        "# Step 3: Load the model weights\n",
        "model_albert.load_state_dict(torch.load(\"/content/drive/MyDrive/model/albert/pytorch_model.bin\"))\n"
      ],
      "metadata": {
        "id": "pwEFsEIRfhUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}